{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo:\n",
    "- Fix scrapper exception bug\n",
    "- Loop through users if fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import subprocess\n",
    "import names\n",
    "from selenium.webdriver.common.by import By\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"maskradiogreece@gmail.com\"\n",
    "password = \"code12345\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_to_number(x):\n",
    "    number = 0\n",
    "    num_map = {'K':1000, 'M':1000000, 'B':1000000000}\n",
    "    if x.isdigit():\n",
    "        number = int(x)\n",
    "    else:\n",
    "        if len(x) > 1:\n",
    "            number = float(x[:-1]) * num_map.get(x[-1].upper(), 1)\n",
    "    return int(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_unlabeled_images(folder_name):\n",
    "    regex = re.compile(r'^.[^\\.].')\n",
    "    \n",
    "    files = []\n",
    "    for file in os.listdir(f\"./{folder_name}\"):\n",
    "        files.append(file)\n",
    "\n",
    "    files_to_clear = list(filter(regex.search, files))\n",
    "    for file in files_to_clear:\n",
    "        os.remove(f\"./{folder_name}/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_profile(name,count):\n",
    "    try:\n",
    "        command = \"instagram-scraper --user \" + name + \" -u \" + email + \" -p \" + password + \" -m \" + str(count) + \" --cookiejar cookies\" \n",
    "        subprocess.check_output(command)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise Exception(\"Sorry, papala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_profile(folder_name):\n",
    "    file = open(f'./{folder_name}/{folder_name}.json', encoding=\"utf8\")\n",
    "    data = json.load(file)\n",
    "    pattern = re.compile(r'\\d+_\\d+_\\d+_n.jpg')\n",
    "    images = []\n",
    "    likes = []\n",
    "    for item in data[\"GraphImages\"]:\n",
    "        finds = re.search(pattern,item[\"display_url\"])\n",
    "        # If the post is at least 2 days old then add it to data or else skip it\n",
    "        if (datetime.today() - datetime.fromtimestamp(item[\"taken_at_timestamp\"])).days > 2:\n",
    "            images.append(finds[0])\n",
    "            likes.append(item[\"edge_media_preview_like\"][\"count\"])\n",
    "    maxLikes = max(likes)\n",
    "    if maxLikes < 500:\n",
    "        file.close()\n",
    "        print(\"DELETING ====> Not Enough Likes\")\n",
    "        shutil.rmtree(rf\"./{folder_name}/\") #removes the folder.\n",
    "        return False\n",
    "    for index,name in enumerate(images):\n",
    "        try:\n",
    "            os.rename(rf\"./{folder_name}/{name}\",rf\"./{folder_name}/{round(likes[index]/maxLikes,2):.2f} {folder_name}{name}\")\n",
    "        except:\n",
    "            pass\n",
    "    file.close()\n",
    "    time.sleep(2)\n",
    "    # Clear Images with no score\n",
    "    print(\"STEP 3 ====> Clearing unecessary images\")\n",
    "    clear_unlabeled_images(folder_name)\n",
    "    # Todo resize all images in folder  \n",
    "    print(\"STEP 4 ====> Moving folder\")\n",
    "    shutil.move(f\"./{folder_name}/\",f\"./Images/{folder_name}/\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open driver and Login to Instagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"./chromedriver_win32/chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "driver.get(\"https://www.instagram.com/\")\n",
    "time.sleep(5)\n",
    "buttons = driver.find_elements_by_tag_name(\"button\")\n",
    "time.sleep(1)\n",
    "buttons[2].click()\n",
    "login_inputs = driver.find_elements_by_tag_name(\"input\")\n",
    "time.sleep(1)\n",
    "login_inputs[0].send_keys(email)\n",
    "time.sleep(1)\n",
    "login_inputs[1].send_keys(password)\n",
    "time.sleep(1)\n",
    "buttons[0].click()\n",
    "time.sleep(5)\n",
    "notification_button = driver.find_elements(By.XPATH, '//button[text()=\"Όχι τώρα\"]')\n",
    "time.sleep(1)\n",
    "notification_button[0].click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0 ====> Waiting 121 seconds\n",
      "STEP 1 ====> Getting 24 from dorothy\n",
      "STEP 2 ====> Evaluating images\n",
      "STEP 3 ====> Clearing unecessary images\n",
      "STEP 4 ====> Moving folder\n",
      "SUM ==============> 24 in total\n",
      "STEP 0 ====> Waiting 123 seconds\n"
     ]
    }
   ],
   "source": [
    "num_images = 0\n",
    "throttle = 600\n",
    "while True:\n",
    "    driver.get(\"https://www.instagram.com/\")\n",
    "    inputs = driver.find_elements_by_tag_name(\"input\")\n",
    "    # The 3rd one is the search \n",
    "    search = inputs[2]\n",
    "    random_female_name = names.get_first_name(gender='female') \n",
    "    search.send_keys(random_female_name)\n",
    "    time.sleep(4)\n",
    "    # Get List of Users\n",
    "    user_list = driver.find_elements(By.XPATH, '//div[@role=\"none\"]')\n",
    "    # Create List \n",
    "    profile_links = []\n",
    "    for user in user_list:\n",
    "        link = user.find_element_by_tag_name(\"a\")\n",
    "        link = link.get_attribute(\"href\")\n",
    "        if link.find(\"explore\") == -1:\n",
    "            profile_links.append(link)\n",
    "    searched_links = pd.read_csv(\"./Links.csv\")\n",
    "    searched_links = searched_links[\"Links\"].to_list()\n",
    "    loop = 1 \n",
    "    for link in profile_links:\n",
    "        name = link.split(\"/\")[3]\n",
    "        if link in searched_links:\n",
    "            print(f\"SKIPPING ====> USER {name} ALREADY SCRAPED \")\n",
    "            continue\n",
    "        driver.get(link);\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            is_private = driver.find_element(By.XPATH, \"//*[text()[contains(., 'Αυτός ο λογαριασμός είναι ιδιωτικός')]]\" )\n",
    "            continue \n",
    "        except: \n",
    "            post = driver.find_element(By.XPATH, \"//*[text()[contains(., 'δημοσιεύσεις') or contains(., 'δημοσίευση')]]\" )\n",
    "            num_posts = int(post.find_element_by_tag_name(\"span\").get_attribute(\"innerHTML\").replace(\",\",\"\"))\n",
    "            if num_posts < 20:\n",
    "                print(\"SKIPPING ====> LESS THAN 10 POSTS\")\n",
    "                continue\n",
    "            followers = driver.find_element(By.XPATH, \"//*[text()[contains(., 'ακόλουθοι')]]\" )\n",
    "            num_followers = convert_str_to_number(followers.find_element_by_tag_name(\"span\").get_attribute(\"innerHTML\").replace(\",\",\"\"))\n",
    "            if num_followers < 3000:\n",
    "                print(\"SKIPPING ====> LESS THAN 3000 FOLLOWERS\")\n",
    "                continue\n",
    "            if loop % 5 == 0:\n",
    "                # Every 5 loop wait between 3-5 min\n",
    "                wait_time = random.randint(180,300)\n",
    "\n",
    "            else: \n",
    "                # Wait between 2-3 min\n",
    "                wait_time = random.randint(120,180)\n",
    "            loop += 1\n",
    "            print(f\"STEP 0 ====> Waiting {wait_time} seconds\")\n",
    "            time.sleep(wait_time)\n",
    "            get_num_posts = math.floor(num_posts * 0.5)\n",
    "            get_num_posts = min(math.floor(num_posts * 0.5),200)\n",
    "            print(f\"STEP 1 ====> Getting {get_num_posts} from {name}\")\n",
    "            try:\n",
    "                scrap_profile(name,get_num_posts)\n",
    "                print(\"STEP 2 ====> Evaluating images\")\n",
    "                added = evaluate_profile(name)\n",
    "                if added:\n",
    "                    num_images += get_num_posts\n",
    "                    print(f\"SUM ==============> {num_images} in total\")\n",
    "            except Exception as e:\n",
    "                print(\"ERROR : \"+str(e))\n",
    "                print(f\"There was error on getting {name}\")\n",
    "                # Sleep bettwen throttle to throttle + 2 minutes\n",
    "                # We get throttling from API\n",
    "                sleep_time = random.randint(throttle, throttle + 60)\n",
    "                print(f\"Going to sleep for {sleep_time}s\")\n",
    "                time.sleep(sleep_time)\n",
    "                # Increase throttle by 1 min\n",
    "                throttle += 60\n",
    "                # Append Failed Link to end of array to try and fetch again \n",
    "                profile_links.append(link)\n",
    "                continue\n",
    "            searched_links.append(link)\n",
    "            df = pd.DataFrame(searched_links,columns=[\"Links\"])\n",
    "            df.to_csv('./Links.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
